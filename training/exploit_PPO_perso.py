from numpy import ndarray

import gymnasium as gym
import json
import torch
from torch.distributions import Categorical
from model_RL.PPO_model_pers import Actor

from mesh_model.mesh_struct.mesh import Mesh
from mesh_model.reader import read_gmsh
from view.mesh_plotter.create_plots import plot_test_results
from view.mesh_plotter.mesh_plots import plot_dataset
from environment.actions.smoothing import smoothing_mean
import mesh_model.random_quadmesh as QM
from environment.gymnasium_envs.quadmesh_env.envs.quadmesh import QuadMeshEnv
import numpy as np
import copy
from tqdm import tqdm


def testPolicy(
        actor,
        n_eval_episodes: int,
        env_config,
        dataset: list[Mesh]
) -> tuple[ndarray, ndarray, ndarray, ndarray, list[Mesh]]:
    """
    Tests policy on each mesh of a dataset with n_eval_episodes.
    :param policy: the policy to test
    :param n_eval_episodes: number of evaluation episodes on each mesh
    :param dataset: list of mesh objects
    :param max_steps: max steps to evaluate
    :return: average length of evaluation episodes, number of wins,average reward per mesh, dataset with the modified meshes
    """
    print('Testing policy')
    avg_length = np.zeros(len(dataset))
    avg_mesh_rewards = np.zeros(len(dataset))
    avg_normalized_return = np.zeros(len(dataset))
    nb_wins = np.zeros(len(dataset))
    final_meshes = []
    for i, mesh in tqdm(enumerate(dataset, 1)):
        best_mesh = mesh
        env = gym.make(
            env_config["env_name"],
            max_episode_steps=30,
            mesh = mesh,
            n_darts_selected=env_config["n_darts_selected"],
            deep= env_config["deep"],
            action_restriction=env_config["action_restriction"],
            with_degree_obs=env_config["with_degree_observation"],
            render_mode="human"
        )
        for _ in range(n_eval_episodes):
            terminated = False
            truncated = False
            ep_mesh_rewards: int = 0
            ep_length: int = 0
            observation, info = env.reset(options={"mesh": copy.deepcopy(mesh)})
            while terminated is False and truncated is False:
                obs = torch.tensor(observation.flatten(), dtype=torch.float32)
                pmf = actor.forward(obs)
                dist = Categorical(pmf)
                action = dist.sample()
                action = action.tolist()
                action_dart = int(action / 4)
                action_type = action % 4
                gymnasium_action = [action_type, action_dart]
                if action is None:
                    env.terminal = True
                    break
                observation, reward, terminated, truncated, info = env.step(gymnasium_action)
                ep_mesh_rewards += info['mesh_reward']
                ep_length += 1
            if terminated:
                nb_wins[i-1] += 1
            if isBetterMesh(best_mesh, info['mesh']):
                best_mesh = copy.deepcopy(info['mesh'])
            avg_length[i-1] += ep_length
            avg_mesh_rewards[i-1] += ep_mesh_rewards
            avg_normalized_return[i-1] += 0 if info['mesh_ideal_rewards'] == 0 else ep_mesh_rewards/info['mesh_ideal_rewards']
        final_meshes.append(best_mesh)
        avg_length[i-1] = avg_length[i-1]/n_eval_episodes
        avg_mesh_rewards[i-1] = avg_mesh_rewards[i-1]/n_eval_episodes
        avg_normalized_return[i-1] = avg_normalized_return[i-1]/n_eval_episodes
    return avg_length, nb_wins, avg_mesh_rewards, avg_normalized_return, final_meshes


def isBetterPolicy(actual_best_policy, policy_to_test):
    if actual_best_policy is None:
        return True

def isBetterMesh(best_mesh, actual_mesh):
    if best_mesh is None or global_score(best_mesh)[1] > global_score(actual_mesh)[1]:
        return True
    else:
        return False


if __name__ == '__main__':


    #Create a dataset of 9 meshes
    mesh = read_gmsh("../mesh_files/medium_quad.msh")
    dataset = [mesh for _ in range(9)]
    with open("../environment/old_files/environment_config.json", "r") as f:
        env_config = json.load(f)
    plot_dataset(dataset)

    env = gym.make(
        env_config["env_name"],
        mesh=mesh,
        max_episode_steps=env_config["max_episode_steps"],
        n_darts_selected=env_config["n_darts_selected"],
        deep=env_config["deep"],
        action_restriction=env_config["action_restriction"],
        with_degree_obs=env_config["with_degree_observation"]
    )

    #Load the model
    actor = Actor(env, 10*8, 4*10, lr=0.0001)
    actor.load_state_dict(torch.load('policy_saved/quad-perso/medium_quad_perso-2.pth'))
    avg_steps, avg_wins, avg_rewards, normalized_return, final_meshes = testPolicy(actor, 15, env_config, dataset)

    plot_test_results(avg_rewards, avg_wins, avg_steps, normalized_return)
    plot_dataset(final_meshes)
    for m in final_meshes:
        smoothing_mean(m)
    plot_dataset(final_meshes)
