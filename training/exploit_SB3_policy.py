import copy
import yaml

import gymnasium as gym
import numpy as np

from tqdm import tqdm
from numpy import ndarray
from stable_baselines3 import PPO

from environment.actions.triangular_actions import flip_edge
from mesh_model.mesh_analysis.global_mesh_analysis import GlobalMeshAnalysis
from mesh_model.mesh_analysis.trimesh_analysis import TriMeshOldAnalysis
from mesh_model.mesh_analysis.quadmesh_analysis import QuadMeshOldAnalysis
from mesh_model.mesh_analysis.trimesh_analysis import TriMeshQualityAnalysis
from mesh_model.mesh_struct.mesh import Mesh
from mesh_model.mesh_struct.mesh_elements import Dart
from mesh_model.random_trimesh import random_mesh, regular_mesh
from environment.actions.triangular_actions import flip_edge_ids, split_edge_ids
from mesh_model.reader import read_gmsh
from view.mesh_plotter.create_plots import plot_test_results
from view.mesh_plotter.mesh_plots import plot_dataset, plot_mesh
from environment.actions.smoothing import smoothing_mean

from environment.gymnasium_envs import trimesh_full_env



def testPolicy(
        model,
        n_eval_episodes: int,
        config,
        dataset: list[Mesh]
) -> tuple[ndarray, ndarray, ndarray, ndarray, list[Mesh]]:
    """
    Tests policy on each mesh of a dataset with n_eval_episodes.
    :param model: the model to test
    :param n_eval_episodes: number of evaluation episodes on each mesh
    :param config: configuration
    :param dataset: list of mesh objects
    :return: average length of evaluation episodes, number of wins,average reward per mesh, dataset with the modified meshes
    """
    print('Testing policy')
    avg_length = np.zeros(len(dataset))
    avg_mesh_rewards = np.zeros(len(dataset))
    avg_normalized_return = np.zeros(len(dataset))
    nb_wins = np.zeros(len(dataset))
    final_meshes = []
    for i, mesh in tqdm(enumerate(dataset, 1)):
        best_mesh = mesh
        env = gym.make(
            config["eval"]["eval_env_id"],
            max_episode_steps=config["eval"]["max_episode_steps"],
            mesh = mesh,
            #mesh_size = 30,
            n_darts_selected=config["eval"]["n_darts_selected"],
            deep= config["eval"]["deep"],
            action_restriction=config["eval"]["action_restriction"],
            with_degree_obs=config["eval"]["with_quality_observation"],
            render_mode = config["eval"]["render_mode"],
        )
        for _ in range(n_eval_episodes):
            terminated = False
            truncated = False
            ep_mesh_rewards: int = 0
            ep_length: int = 0
            obs, info = env.reset(options={"mesh": copy.deepcopy(mesh)})
            while terminated == False and truncated == False:
                action, _states = model.predict(obs, deterministic=False)
                if action is None:
                    env.terminal = True
                    break
                obs, reward, terminated, truncated, info = env.step(action)
                ep_mesh_rewards += info['mesh_reward']
                ep_length += 1
            if terminated:
                nb_wins[i-1] += 1
            if isBetterMesh(best_mesh, info['mesh'], config["env"]["analysis_type"]):
                best_mesh = copy.deepcopy(info['mesh'])
            avg_length[i-1] += ep_length
            avg_mesh_rewards[i-1] += ep_mesh_rewards
            avg_normalized_return[i-1] += 0 if info['mesh_ideal_rewards'] == 0 else ep_mesh_rewards/info['mesh_ideal_rewards']
        final_meshes.append(best_mesh)
        avg_length[i-1] = avg_length[i-1]/n_eval_episodes
        avg_mesh_rewards[i-1] = avg_mesh_rewards[i-1]/n_eval_episodes
        avg_normalized_return[i-1] = avg_normalized_return[i-1]/n_eval_episodes
    return avg_length, nb_wins, avg_mesh_rewards, avg_normalized_return, final_meshes


def isBetterPolicy(actual_best_policy, policy_to_test):
    if actual_best_policy is None:
        return True

def isBetterMesh(best_mesh, actual_mesh, analysis_type):
    tri = False
    for d_info in actual_mesh.dart_info:
        if d_info[0]>=0:
            d = Dart(actual_mesh, d_info[0])
            if d == ((d.get_beta(1)).get_beta(1)).get_beta(1):
                tri = True
            else:
                tri = False
            break
    if tri:
        if analysis_type == "old":
            ma_best_mesh = TriMeshOldAnalysis(best_mesh)
            ma_actual_mesh = TriMeshOldAnalysis(actual_mesh)
        else:
            ma_best_mesh = TriMeshQualityAnalysis(best_mesh)
            ma_actual_mesh = TriMeshQualityAnalysis(actual_mesh)
    else:
        ma_best_mesh = QuadMeshOldAnalysis(best_mesh)
        ma_actual_mesh = QuadMeshOldAnalysis(actual_mesh)
    if best_mesh is None or ma_best_mesh.global_score()[1] > ma_actual_mesh.global_score()[1]:
        return True
    else:
        return False

if __name__ == '__main__':

    #Create a dataset of 9 meshes
    mesh = read_gmsh("../mesh_files/tri-star.msh")
    # ma = TriMeshQualityAnalysis(mesh)
    # split_edge_ids(ma, 5, 2)
    # split_edge_ids(ma, 2, 10)
    # flip_edge_ids(ma, 3,7)
    # #plot_mesh(mesh)

    dataset = [mesh for _ in range(1)]
    # PARAMETERS CONFIGURATION
    with open("../training/config/trimesh_config_PPO_SB3.yaml", "r") as f:
        config = yaml.safe_load(f)
    plot_dataset(dataset)

    #Load the model
    model = PPO.load("policy_saved/tri-sb3/PPO_SB3_tri-delaunay-v0.zip")
    avg_steps, avg_wins, avg_rewards, avg_normalized_return, final_meshes = testPolicy(model, 1, config, dataset)

    plot_test_results(avg_rewards, avg_wins, avg_steps, avg_normalized_return)
    plot_dataset(final_meshes)
    for m in final_meshes:
        smoothing_mean(m)
    plot_dataset(final_meshes)