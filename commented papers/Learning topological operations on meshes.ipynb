{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learning topological operations on meshes with application to block decomposition of polygons\n",
    "\n",
    "A.Narayanan, Y.Pan, P.-O. Persson"
   ],
   "id": "d201879b906bff30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Objectif :** Améliorer la régularité des maillages, en se concentrant sur la connectivité des maillages. L’idée est de minimiser les irrégularités sur les sommets.\n",
    "\n",
    "Ce papier se base sur des études de maillages 2D triangulaires et quadrangulaires.  Dans la suite sera traitée uniquement la partie maillage triangulaires.\n",
    "\n",
    "Les nœuds d’un maillage disposent d’un certain nombre d’arêtes incidentes. Ceci est une propriété principale de la régularité d’un maillage. En effet, chaque sommet a un nombre d’arêtes incidentes idéal."
   ],
   "id": "a430e41ed09bcab9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "- Pour les nœuds internes :  $360/60° = 6$\n",
    "- Pour les noeuds externes : $ max ([\\theta/\\alpha] +1, 2) = 2,3,4 \\dots $"
   ],
   "id": "7ea13786011a9ff6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si ce nombre n’est pas atteint, il existe alors des irrégularités. L’objectif étant d’avoir le moins d’irrégularités possibles, un global score peut alors être défini :\n",
    "\n",
    "* $s= \\sum_1^{N_v} | \\Delta i | $  \n",
    "\n",
    "* $s*= | \\sum_1^{N_v} \\Delta i | $"
   ],
   "id": "b24b9fd51d37f07e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "De plus, afin d’améliorer la connectivité, dans le cas des maillages triangulaires il existes trois actions possibles :\n",
    "- Edge Flip \n",
    "- Edge Split\n",
    "- Edge Collapse"
   ],
   "id": "cc0135fb57da3162"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Afin de représenter ces maillages, la structure **DCEL** (doubly connected edge list) est utilisée.\n",
    "\n",
    "Afin de générer une base de maillage à étudier, les méthodes « Delaunay refinement » sont appliquées. Ces dernières ne permettent pas de définir des structures de connectivités spécifiques. C’est pourquoi, ce travail cherche à améliorer la connectivité des ces maillages en utilisant un algorithme d’apprentissage par renforcement.\n",
    "\n",
    "Le problème peut être modélisé comme un **Markov Decision Process**, il s’agit d’une modélisation adaptée lorsque l’environnement est connu (nombre fini d’état, d’actions possibles, et de récompenses).\n",
    "Selon l’état dans lequel on se trouve, une action différente sera choisie et donc une récompense différente.  \n",
    "MDP involve delayed reward ⇒ il faut donc gérer les récompenses immédiates et futures.\n",
    "Ce qui peut se modéliser comme suit :"
   ],
   "id": "c4e0b46bb270166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:10:09.901879Z",
     "start_time": "2024-04-08T14:10:09.885727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"Schema.jpg\", width=300, height=200)"
   ],
   "id": "471c350686a753ae",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Schema.jpg\" width=\"300\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Après chaque action, la récompense obtenue sera : $rt = st-st+1$\n",
    "\n",
    "Lorsque le maillage a subit t tranformations et est alors à l’état Mt, le discounted return est defini comme suit :\n",
    "\n",
    "$Gt = \\sum_{k=t}^{n} \\gamma^{k-t} r_k  $\n",
    "\n",
    "Le « jeu » s’arrête lorsque $st=s*$, c’est à dire que le maillage idéal est atteint, ou que le nombre de pas maximal a été atteint.\n",
    "\n",
    "Pour entraîner le modèle, l’algorithme de Proximal Policy Optimization est utilisé. Il s’agit d’un algorithme on policy. Classé comme une policy gradient methode, il s’agit d’une version sophistiquée de la méthode REINFORCE"
   ],
   "id": "1f49d68e865f8b5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Opération de **CONVOLUTION** \n",
    "\n",
    "Les opérations de convolutions permettent d'encoder les informations topologiques autour de chaque brin. En répétant ces opérations, les informations topologiques de tous le maillage sont encodées, et ce de manière expensive.\n",
    "\n",
    "L'état de chaque brin permet de représenter la topologie locale.\n",
    "\n",
    "SCHEMA\n",
    "\n",
    "Les caractéristiques topologiques de chaque brin sont ensuite stockées dans une matrice caractéristique x :\n",
    "\n",
    "SCHEMA \n",
    "\n"
   ],
   "id": "a194972f2bbe4dc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Résultats :**\n",
    "\n",
    "Le modèle a été entraîné sur 100 rollouts, de plus, les performances ont été mesurées sur 100 maillages.\n",
    "Tout d’abord ils ont obtenu une performance de 0.81 (sigma=0.11). Mais comme la politique suivie est stochastique (basée sur des probabilités) chaque maillage généré sera différent. Il convient alors de choisir le meilleur maillage après k runs. Avec cette dernière méthode en fixant k=10, les performances s’améliorent: 0.86 (sigma=0.08).\n",
    "\n",
    "A noter que le temps de convergence à la bonne politique, et  pour atteindre un bon maillage augmente avec la taille des maillages. Cela est à prendre en compte dans la lecture des résultats."
   ],
   "id": "27eac32fe8c24f82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cd7af34f523cd190"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Questions :**\n",
    "\n",
    "A quoi correspond la advantage function ? Elle est équivalente à la fonction de valeur d’état v qui est aussi commune en apprentissage par renforcement.\n",
    "\n",
    "Pourquoi utiliser un « discounted return » et considérer la tâche comme continue et non pas comme des épisodes ? Pour plus prendre en compte les dernières actions.\n",
    "\n",
    "Qu’est ce que la topologie ?\n",
    "Comment sont construits exactement les templates ?\n",
    "\n",
    "Faire des recherches sur Monte Carlo Search Tree ⇒ Dans le book de RL e revoir les notions de loss functions, et d’entropy"
   ],
   "id": "21838aa4c59cf53d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
